{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is : Fri Dec  6 19:15:52 2019\n",
      "(334295, 3800)\n",
      "334295\n"
     ]
    }
   ],
   "source": [
    "#  This file experiments with Sklearn's logistic regression model and tuning the hyperparameters for the \n",
    "#     best (within the time constraints) preforming model as determined by cross-validation\n",
    "\n",
    "import scipy.sparse\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time;\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#  Load the training instances and corresponding labels\n",
    "trainset = scipy.sparse.load_npz('../data/corpus_feature_vectors.npz')\n",
    "labels = []\n",
    "with open('../data/corpus_labels.csv', encoding='utf-8') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        labels.append(row[0])\n",
    "\n",
    "print(trainset.shape)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Using Sklearn's standardization class, we normalize the training set feature values\n",
    "#      so that no one feature is unfairly weighted due to scale differences.\n",
    "#   Without this step, the accuracy of our models went down\n",
    "sc = StandardScaler(with_mean=False)\n",
    "sc.fit(trainset)\n",
    "normalized_trainset = sc.transform(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for default model without any hyperparameter tuning\n",
    "model_default = LogisticRegression(class_weight='balanced', random_state=0, solver='saga', max_iter=200, multi_class='multinomial')\n",
    "#model_default.fit(normalized_trainset, labels)\n",
    "\n",
    "# Training for tuned model #1\n",
    "# Differences:\n",
    "#    Using elasticnet instead of l2 for regularization\n",
    "#    Using l1 ratio 0.9\n",
    "model_1 = LogisticRegression(penalty='elasticnet', class_weight='balanced', random_state=0, solver='saga', max_iter=200, multi_class='multinomial', l1_ratio=0.9)\n",
    "#model_1.fit(normalized_trainset, labels) # so far the best\n",
    "\n",
    "\n",
    "# Training for tuned model #2\n",
    "# Differences:\n",
    "#    Non-balanced training set\n",
    "#    Using l1 ratio 0.9\n",
    "\n",
    "model_2 = LogisticRegression(random_state=0, solver='saga', max_iter=200, multi_class='multinomial')\n",
    "#model_2.fit(normalized_trainset, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6420676348733902\n"
     ]
    }
   ],
   "source": [
    "#   A quick test on 80% of the train data for training and testing on the 20% held-out \n",
    "test_size=0.2\n",
    "\n",
    "feats_train, feats_test, labels_train, labels_test = train_test_split(normalized_trainset,\n",
    "                                                                     labels,\n",
    "                                                                     test_size=test_size)\n",
    "model_1.fit(feats_train, labels_train)\n",
    "print(model_1.score(feats_test, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 3-fold CV for default:  Fri Dec  6 05:02:42 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kalee\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 3-fold CV for default:  Fri Dec  6 05:09:25 2019\n",
      "[0.64223345 0.64022004 0.64374944]\n",
      "Started 3-fold CV for model_1:  Fri Dec  6 05:09:25 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kalee\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 3-fold CV for model_1:  Fri Dec  6 07:04:17 2019\n",
      "[0.64229627 0.64045337 0.64391098]\n",
      "Started 3-fold CV for model_2:  Fri Dec  6 07:04:17 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kalee\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started 3-fold CV for model_2:  Fri Dec  6 07:10:23 2019\n",
      "[0.66913751 0.67060629 0.67119268]\n"
     ]
    }
   ],
   "source": [
    "#   Used 3-fold cross validation to get a better accuracy measure of the logistic regression models with different hyperparameters\n",
    "\n",
    "print (\"Started 3-fold CV for default: \", time.asctime( time.localtime(time.time()) ))\n",
    "# Evaluation of default model\n",
    "scores = cross_val_score(model_default, normalized_trainset, labels, cv=3)\n",
    "print (\"Finished 3-fold CV for default: \", time.asctime( time.localtime(time.time()) ))\n",
    "print(scores)\n",
    "\n",
    "print (\"Started 3-fold CV for model_1: \", time.asctime( time.localtime(time.time()) ))\n",
    "# Evaluation of model_1 to check if overfitting\n",
    "scores = cross_val_score(model_1, normalized_trainset, labels, cv=3)\n",
    "print (\"Started 3-fold CV for model_1: \", time.asctime( time.localtime(time.time()) ))\n",
    "print(scores)\n",
    "\n",
    "print (\"Started 3-fold CV for model_2: \", time.asctime( time.localtime(time.time()) ))\n",
    "# Evaluation of model_1 to check if overfitting\n",
    "scores = cross_val_score(model_2, normalized_trainset, labels, cv=3)\n",
    "print (\"Started 3-fold CV for model_2: \", time.asctime( time.localtime(time.time()) ))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Saved the trained model in a pickle file\n",
    "import pickle\n",
    "fileObject = open(\"pickled_logistic_regression_model\", 'wb')\n",
    "pickle.dump(model_1, fileObject)\n",
    "fileObject.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

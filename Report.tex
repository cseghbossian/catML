\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{titling}
\usepackage[legalpaper, portrait, margin=1in]{geometry}

\setlength{\droptitle}{-8em}   % This is your set screw
\title{Predicting Star Ratings from User Reviews\\CSE 142 Fall 2019}
\date{December 6, 2019}
\author{Group \#2: Angelina Agabin (aagabin - 1590090)  \\ Celine Seghbossian (cseghbos - 1595557) \\ Kaleen Shrestha (kashrest - 1615749)}
\begin{document}

\maketitle

\section{Tools Used (can be reduced if we go over the 3 page limit)}
We extensively made use of packages in NLTK for data pre-processing, Scikit-learn for feature extraction,  and ML algorithms, specifically:\\
\\sklearn.feature\_extraction.text import TfidfVectorizer: This tool made use of the bag of word model by taking documents and extracting n-gram features. The hyper parameters were tuned to make use of the term-frequency - inverse document frequency (TF-IDF) to pick weights for features, favoring features/words that occur frequently within a single document but not frequently across all documents.\\
\\nltk.stem import PorterStemmer: This tool was used to pre-process the text data by stemming words. An example would be that texts such as run, running, runs, runned would all be transformed to run.\\
\\sklearn.preprocessing import StandardScaler: This was used to normalize feature vectors so that no feature (like the word "the") would be unfairly favored due to scale imbalance.\\
\\sklearn.model\_selection import train\_test\_split: This class was used to do cross-validation to gather a better accuracy/evaluation of current model to help with tuning the hyper-parameters of the models.\\
\\sklearn.linear\_model import Perceptron, LogisticRegression, svm.LinearSVC, from sklearn.neighbors import NearestCentroid: These were imported models, we used three linear models and one variante of knn from sklearn, and experimented tuning the hyperparameters to get good preforming instances of the models.

\section{Diversity}
Add later
\section{Abstract}
\section{Data Pre-processing}
\section{Feature Extraction}
\section{Approaches}
Everyone: Just describe each model and any cool features that made you chose the model, or something. Also we can mention voting as a way of combining the predictions (ensemble).
\section{Results}
Everyone: Add some accuracy measurements of the models you tested, at least one accuracy measure using CV before tuning, then another accuracy measure using CV after tuning.
\section{Conclusion}
Good work everyone!
\section{Ideas for Future Work}
More models, using Ada-boost or XG-boost for the weak-preforming models, etc.
\end{document}
